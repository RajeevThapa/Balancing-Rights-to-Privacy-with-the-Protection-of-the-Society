\chapter{Threat Landscape Impacting Privacy} \label{cha:Threat landscape that impacts privacy}

% Please, Include the following:
% \\\\
% taxonomy and examples (ransomware, spyware, data breaches, insider leaks, surveillance, AI inference)
The modern privacy threat landscape is broad and dynamic, attacks that used to target availability or financial assets now regularly the theft or misuse of personal data. New technologies introduced new methods for inference and surveillance which has increased the risks that extend beyond traditional cybersecurity concerns. To understand this landscape requires taxonomy (what types of threats exist) and also an explanation of how each threat presents itself in specific harms for individuals such as loss of confidentiality, reputational damage, misuse personal information, adverse effects on free expression, and loss of autonomy \cite{ENISA2024ThreatLandscape} \cite{regulation2016regulation}. 
The following section covers the major threat families that threaten individual privacy in a digitalized society. Each threat type is discussed in terms of its technical operation, representative incidents, and specific privacy implications under the EU’s GDPR and the NIS2 Directive. When these categories are combined, they show how cyber risks affect trust and integrity in data ecosystems in multiple ways. \\

\noindent
\textbf{Data breaches and unauthorized disclosures}: One of the most common and frequently categories of privacy-impacting threats is data breaches and unauthorized disclosures which involves the exposure of personal data through hacking, misconfiguration, or human error. Technically, adversaries exploit vulnerabilities in authentication mechanisms, cloud storage configurations, or web application code, including SQL injection and privilege escalation. A recent IEEE study found that 60 percent of major breaches between 2020 and 2024 resulted from misconfigured cloud storage or compromised credentials \cite{shbair2023cloud}. A high-profile example is the 2021 Facebook information leak, which affected over 500 million users. It revealed exposed metadata such as phone numbers and email addresses \cite{abawajy2023social}. The privacy implications extend beyond instant data exposure, re-identification attacks, identity theft, and long-term monitoring become possible if personal information is collected and circulated in dark web. Under GDPR, it requires data breaches to be reported within 72 hours, and NIS2 expands this responsibility to a broader set of critical and relevant entities, focusing resilience and incident management \cite{EDPB2022GuidelinesBreachNotification} \cite{schmitz2023defining}.  Another threat category involves ransomware and extortion-based attacks, which combine data encryption with threats to release personal or corporate information unless ransom demands are met. Modern ransomware strains, such as Ryuk or LockBit, frequently adopt double extortion tactics, simultaneously encrypting and exfiltrating data to put pressure on victims \cite{conti2021ransomware}. Even if systems are restored from backups, these attacks have serious privacy implications. Copied data can be disclosed or sold on dark web markets also in chat platform like telegram and signal which permanently compromising people's personal and health information. For privacy impact analysis, businesses must emphasize the downstream effects on data subjects (identity theft, harassment, healthcare risks if medical records are leaked) as well as the legal process in which organizations must coordinate GDPR breach notifications with operational incident response \cite{regulation2016regulation}. The NIS2 directive specifically classifies ransomware as a major operational risk which requires member states to increase both preventive and reactive procedures. \\

\noindent
\textbf{Social engineering and Phishing}:Phishing and social engineering strategies are among the most common methods of breaching privacy. According to research, phishing primarily targets human psychology rather than technology vulnerabilities. A 2024 study of compliance principles applied to phishing emails found that distraction, deception, and control are the most commonly used persuasion methods. These approaches exploit victim's interest and confidence, trick them to click links, share credentials, or disclose personal information. A study of phishing research from 2006 to 2024 shows a considerable shift from only technological detection methods and importance toward the studies on human behavior and awareness \cite{desolda2024phishinghumanfactor}. Techniques have evolved from generic mass emails to highly customized or designed "spear phishing" operations, accompanied by AI-generated messages that resemble normal communication patterns, free of errors.\cite{zhang2020phishing}.For example, attackers are using crafted e-signature request to higher level employees to trick them into clicking the links and provide information. Furthermore , attackers are becoming more sophisticated in phishing by compromising an organization's service email accounts which are usually without MFA for convenience and using them to send phishing emails that appear legitimate to recipients. As the messages come from verified accounts within the business, this technique compromises established technical protections like spam filters and domain verification. Once an attacker has gained access, they can elevate privileges, steal sensitive data, or move laterally across connected computers. Although multifactor authentication, email filtering, and threat intelligence systems provide important layers of defense, they only address part of the problem. The persistence of phishing attacks demonstrates that data protection is as much a socio-behavioral challenge as it is a technical one. 
\\
\noindent Regulatory frameworks the GDPR says, organization must implement appropriate technical and organizational measures (Article 32), which increasingly include user education and phishing-resistant authentication mechanisms such as FIDO2 and the NIS2 emphasizes accountability, awareness training, and the human element in security governance. The latest research reinforces this approach by examining continuous phishing simulations, adaptive awareness programs, and proactive monitoring of internal communication patterns. As phishing continues to evolve through automation, deception, and human manipulation, organizations must combine technical safeguards with sustained behavioral interventions to build resilience against this dynamic and adaptive threat \cite{ENISA2024ThreatLandscape}. \\

\noindent
\textbf{Internet-of-Things (IoT)}:Internet-of-Things (IoT) devices and edge/fog computing introduce a growing number of physical sensors and constrained devices that continuously capture personal and contextual data which introduces a multidimensional privacy risk. IoT systems, often designed with limited processing power and minimal encryption, provide attackers with an expansive attack surface for data interception ,re identification and device manipulation \cite{islam2022iot}. Studies shows that poorly secured sensors in smart homes or healthcare systems can enable inference of user behavior pattern recognition, even when explicit data is anonymized \cite{sharma2023privacy}. Furthermore, In the online space, personal privacy is frequently threatened through various forms of individual network behavior and data sharing. Beyond the mere exposure of personal information, the IoT is capable of fully perceiving environmental details using technologies such as cameras, global positioning systems (GPS), sensors, and RFID devices. This enables the IoT to gather comprehensive information regarding the physical environment, behavioral states including location and duration of stay, and physiological conditions of individuals. For privacy, this means that individuals are subject to tracking, behavioral profiling, location tracing, and inference without their knowledge, making the concept of minimum data collection more difficult to implement. 
\\
\noindent Legal frameworks such as GDPR focus, data minimization, and transparency, but many IoT systems struggle to meet these standards. NIS2 highlights these risks indirectly by emphasizing supply-chain security and vendor responsibility, which are especially crucial in IoT installations because of it's fragmented and opaque nature.Privacy risks in IoT environments therefore arise from both technical weaknesses and systemic governance gaps.\cite{ENISA2024ThreatLandscape}. \\

\noindent
\textbf{Supply-chain and third party}: Supply-chain and third-party breach are systemic sources of privacy risk. Adversaries utilize trusted vendor connections, software libraries, or hardware components to gain privilege and access across different companies \cite{schneier2023supplychain}. A malware-infected vendor update or compromised code may spread widely before detection. The 2020 SolarWinds breach shows this pattern which compromised updates spread malicious code to thousands of businesses, including EU agencies. 
\\

\noindent Under GDPR, data controllers are responsible for ensuring that processors and third parties offer acceptable data protection standards. Under NIS2, supply-chain risk management is now a key responsibility for companies providing essential services, and under GDPR, data controllers and processors must ensure that processors provide equal protection of personal data. The combination of technical supply-chain risk and regulatory duties means that privacy protection cannot be managed only through internal measures \cite{kostyuk2024supplychain}. \\

\noindent
\textbf{Tracking and Surveillance}:
Metadata, traffic-analysis, and inference attacks highlight a more hidden but equally dangerous threat to privacy. Even when content is encrypted, metadata (who, when, where, and how often) and side channels (timing, volume, behavioral telemetry) can allow for profiling, location inference, membership inference, or participation in sensitive groups \cite{singla2024metadata}. For example, researchers have shown that traffic patterns in encrypted network flows can disclose user identities and behavioral routines with more than 90 percent accuracy \cite{bianchi2024inference}. One of the highly controversial, the EU’s “Chat Control” proposal in May 2022 , regulation to prevent and combat Child Sexual Abuse (CSAR)\cite{europeancommission2022childabuse}. This legislation mandates interpersonal communications service providers within the EU to scan users' private messages, images, and videos, including those using end-to-end encryption, for both known and unknown child sexual abuse material (CSAM) or grooming behavior. The scanning responsibility changes the service provider's function from a mere transport channel to a content observer. This involves more than simply collecting information or logs; the service must implement advanced algorithms for semantic analysis, pattern matching, and machine-learning recognition of grooming or CSAM material. As a result, user activities such as conversation, image sharing, link exchanges, and file uploads are automatically scrutinized, increasing the risk of profiling. Individuals could be marked, sorted, or filtered based on their communication habits, such as the frequency with which they engage and the types of attachments they share, even without specific suspicion. This profiling creates serious privacy problems. In terms of legal implications, the Chat Control proposal overlaps with the rights protected by the GDPR and the NIS2 Directive in several ways. 
\\

\noindent According to GDPR, Articles 7 and 8 of the EU Charter of Fundamental Rights protect the right to privacy and the protection of personal data. Furthermore, if automated scanning methods result in the classification or profiling of users based on communication patterns, GDPR duties under Articles 22 (automatic decision-making), 5(1)(a) (lawfulness, fairness), and 5(1)(c) (data minimization) become applicable. Under NIS2, if such communications services are classified as essential or important entities (for example, messaging providers used in critical infrastructure), their risk-management frameworks must account for large-scale processing of personal communications data as well as potential security incidents such as unauthorized profiling or data misuse. \cite{freiheitsrechte2022chatcontrol} \cite{androuet2025chatcontrol} \cite{hintersdorf2022clientside}. 
\\

\noindent \textbf{AI driven privacy threats}:
Systems based on AI have created a new class of privacy risks which is different than traditional data-exfiltration or malware incidents because they can create sensitive information through inference and indirectly leak sensitive training data through model behavior. At the technical level, three interconnected attack families are present as per current research and incident reporting: \\

\noindent
 \begin{enumerate}
     \item \textbf{Training-data extraction and memorization}, where models (especially very large ones) unintentionally reproduce fragments of their training data in responses to output. The privacy risk is that when a model generates a training sample containing personally identifiable information (PII), the PII is publicly exposed via the model interface or API. 
     
     \noindent Under GDPR, outputs containing personal data can impose obligations on controllers and processors: the leak of personal data via a model response can be considered a personal data breach (Article 33) if it is the result of insufficient technical or organizational measures and violates data-subject rights such as the right to erasure and the right to be informed \cite{yao2024llmsecurityprivacy}.
     \item \textbf{Membership inference and model inversion}, in which attackers determine whether a specific individual's data were used to train a model or reconstruct properties of private inputs. These strategies make use of overfitting, model overconfidence, and the distributional differences between training and non-training inputs. For example, in a health care environment, if a model reveals that a patient record contributed to training, it could suggest that the individual had a particular illness or was treated at a specific institution. Surveys and comprehensive investigations have confirmed the effectiveness of such attacks across classifier models and domains \cite{bai2024membership}. 
     \\
     \noindent When models allow identification of training participation, the right to data minimization (Art. 5(1)(c)) and integrity and secrecy (Art. 5(1)(f)) are violated. Controllers must examine whether such inference constitutes "processing" of personal data. Many experts believe it does, and hence the controller must use DPIAs (Art. 35) for high-risk models. NIS2 requires organizations to manage cyber risks, which includes the misuse of artificial intelligence systems that promote data leakage, indicating that inference assaults should be incorporated in risk-assessment and security governance processes.
     \item \textbf{Human-AI interaction leakage} When humans provide sensitive data to an AI system (by conversation, upload, or associated tools), which may then retain or handle this data in ways that pose a privacy risk. For example, users may communicate medical issues, workplace secrets, or personal identifiers to an AI assistant. The assistant may log or reuse that information, or the conversational context may be exposed unintentionally through model memorization or attackers can craft prompts that induce models to share hidden instructions, training artifacts, or integrated consumer data. Because users frequently feel they are communicating with a conversational tool rather than a data-processing service.
     \\
     \noindent Here the GDPR's transparency and consent requirements (Articles 6, 7) may be undermined. In many deployments, the user is not properly informed that the communication may be saved, repurposed for training, or made available to others, which violates the GDPR's accountability and transparency requirements (Art. 24, Art. 12). Furthermore, organizations deploying such systems may fail to recognize the human-AI interaction as a "processing activity" that requires a DPIA. Under NIS2, if the service is critical, the logging and analysis of conversational data must be integrated into the risk-management system, but many organizations fail to do so due to a lack of specific standards. Key issues include unclear roles between “data controller” and “processor,” challenges in obtaining lawful bases for extensive training datasets, and difficulties in complying with GDPR Art. 17 regarding the removal of user data from models. Additionally, there is a lack of clear incident categorization for AI model leakage under NIS2. Despite existing technical counter-measures, many organizations struggle to integrate them with governance frameworks. The absence of specific regulatory guidance for AI privacy risk contributes to inconsistent practices and privacy vulnerabilities\cite{liu2024generative} \cite{lukacs2023gdpr}.
 \end{enumerate}

\noindent
These threat groups shows that the digital age's loss of privacy is caused by both malicious exploitation, justified excessive data gathering, and AI-driven privacy concerns present an important change in how personal data can be exposed. Regulatory frameworks must adapt to address model-based leakage, human-AI interaction risks, and supply-chain opacity in AI pipelines. There is a delicate balance between using AI for social good, such as threat detection and health analytics, and safeguarding individual rights by GDPR and fulfilling obligations under NIS2 \cite{conti2024nis2}.  To achieve this,  a comprehensive, multi-layered strategy is essential, integrating cryptographic PETs, clear user engagement, robust governance, and explicit allocation of legal responsibilities.

